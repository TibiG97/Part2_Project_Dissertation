\begin{document}

\chapter{Introduction}

This project aims to make use of, re-implement and adapt machine learning techniques in order to identify, classify, and rename files from distributed systems, which end up having un-intuitive names. These names can be the result of either intentional activity (e.g. be created by attackers to disguise the actual contents of files before exfiltration, concatenate data from multiple files before exfiltration) or appear organically as part of the using system (e.g. running of scripts and experiments that use temporary file names which are not deleted, files produced by the simulated developers when downloading documentation or new software from the internet). 


\section{Aims and Motivation}  \label{1.1}

In order to develop a set of features for the intended classification pipeline, the project will use a couple of different sources of information about the target files. Firstly, a typical approach involving file classification with respect to their content is implemented. Secondly, to generate further features, and as an element of novelty, I will use system traces which highlight the relation between programs and data objects, stored in the format of a graph database. In these \textit{provenance graphs}, nodes represent entities such as files, processes, sockets, pipes, while edges are recording, at a fine grained level, influences between the nodes. Thus, at the core of the project there is a graph classification pipeline. Specifically, the Patchy-San$^{\small \cite{Patchy-San}}$ algorithm is used in combination with a convolutional neural network to achieve the desired result. As a final stage of the project, I have implemented files re-naming based on user-given policy and by using the acquired classification information. The renaming proposals are firstly presented to the used, as opposed to intervening on the files directly. \\

The provided provenance dataset is a result of the \textit{Observational Provenance in User Space} (OPUS) research project$^{\small \cite{OPUS}}$, and these system traces are taken from machines on which typical developer/attacker behaviour is simulated. Thus, while everything is made to look similar to normal user behaviour, these are not genuine activities or sensitive data being exfiltrated. Hence, neither privacy nor ethics are a primary concern. \\

The point of the project is to explore how/with what degree of success can machine learning be deployed to deal with the classification/naming issues mentioned above. Not applying machine learning means one has to know a lot more about what executes on the system and the types of files or the types of activity that generates them. The exercise here is to see how well a ML algorithm can start reducing the amount of knowledge needed from analysts when looking at an arbitrary system. In short, it is a trade-off: one can apply non-ML approaches on this particular dataset if you start by knowing the ground truth or the types of activities that run on the system, but it's unclear whether a ML approach could do just as well while requiring less anterior knowledge. The goal is to design a ML pipeline to test this hypothesis out and give an answer to the question. 

\section{Challenges}

One obstacle was the lack of enough provenance training data and examples of suggestible patterns that could be fed as input to the ML models. To overcome this, synthetic data was created by generating variations of graphs which mimic the original dataset. This process is described in detail in Section \ref{Synthetic Data Generation}.
From now on, the term \textit{real data} will refer to OPUS generated graphs, while \textit{synthetic data} to these newly created datasets. 

\section{Related Work}

This section discusses various approaches on individual sub-tasks of the project. Each of the ideas presented is contrasted with the project's requirements, explaining whether it is a viable option for its purposes or not. 

\subsection{Document Classification}

Document Classification is an important and typical tasks in supervised machine learning. Assigning categories to documents, which can be a web page, library book, media articles, gallery etc. has many applications, e.g. spam filtering, email routing, sentiment analysis. The bag-of-words$^{\small \cite{BOW}}$ model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier such as Naive Bayes$^{\small \cite{NaiveBayes}}$ or Support Vector Machines$^{\small \cite{SVM}}$. This model is a simplifying representation used in natural language processing in which a text (such as a sentence or a document) is converted into a bag (multiset) of its own words, disregarding grammar and even word order, but keeping multiplicity. \\

This approach has been successfully used for human written documents. However, my project rather aims to classify file generated by various programs and processing pipelines, which will probably require different or improved techniques. \\

\subsection{File Renaming using Metadata}

There exists a large variety of tools designed to automatise the file renaming task. One such tool called \textit{Rename Expert} implements a considerable amount of features of interest. For instance:

\begin{itemize}
  \item metadata (EXIF, IPTC, ID3, etc.) can be used for the naming of files (e.g., for images, video files or audio files).
  \item automatically rename text files based on a part of the file contents (e.g., to include data from TXT, XML, HTML, or log files).
  \item simultaneously rename folders along with subfolders and files - allows the batch renaming of any number of files and folders in one go.
\end{itemize}

The downside of this category of programs is that they require extensive user involvement and detailed knowledge of the target system. In contrast, the project desires to reduce as much as possible the effort and contribution of a specialist by using appropriate machine learning techniques. Moreover, this project is inclined towards more generality, being less dependent of files' metadata. 


\subsection{Graph Kernels}

Graph Kernels$^{\small \cite{GraphKernels}}$ make kernel machines such as SVMs feasible for graph classification by computing some positive semidefinite graph similarity measures, which have achieved state-of-the-art classification results on many graph datasets. Most existing graph kernels focus on comparing small local patterns. Deep graph kernels learn latent representations of substructures (walks, paths, subtrees) to leverage their dependency information. Convolution kernels compare two graphs based on all pairs of their substructures. Assignment kernels, on the other hand, tend to find a correspondence between parts of two graphs. \\

In contrast to these computationally expensive approaches, the proposed Patchy-San algorithm is time efficient and highly parallelisable, as will be discussed in Section \ref{Evaluation Results}.

\end{document}