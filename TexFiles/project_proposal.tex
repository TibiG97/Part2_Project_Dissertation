
\begin{document}

\chapter{Project Proposal}
\begin{center}
  \Large
  Computer Science Tripos -- Part II -- Project Proposal\\[4mm]
  \LARGE
  Descriptive file names from content and context \\[4mm]

  \large
  Candidate 2347C

  Originator: Dr Lucian Carata

  19$^{th}$ October 2018
\end{center}

\vspace{5mm}

\textbf{Project Supervisor:} Dr Lucian Carata

\textbf{Director of Studies:} Dr John Fawcett

\textbf{Project Overseers:} Dr Simone Teufel  \& Dr Andrew Rice

% Main document

\section*{Introduction}

\subsubsection*{Description}

This project addresses the problem of files that are created either by users or by processes among which many of them end up having extremely unintuitive or non-descriptive names (e.g. foo, bar, stdin, stdout). Specifically, based on the contents of the file and the context within which it was created the project aims to classify and rename/reorganize files by providing users with a virtual view of the file system. This view (a virtual file system) is built based on both the classification and user-given policy.

\subsubsection*{Dataset}

The target dataset consists of system traces taken from machines on which typical developer/attacker behaviour and work-flows are simulated. Furthermore, the data is synthetically generated to mimic real activity (i.e. it is not a simple script to repeat things in a loop), with attacks performed by a "red team" (i.e. a team specialised in attacking systems) as part of a DARPA project on Transparent Computing. \\
In terms of the format of the data I will be working with, these traces are already converted into a graph database according to a well defined model called OPUS/PVM (\url{https://www.cl.cam.ac.uk/research/dtg/fresco/opus}). In the mentioned graphs the nodes represent precesses, files, sockets and other system-level entities, while the edges represent influences and data-flow. The target files can be human-written documents, but most would be outputs generated by various programs and processing pipelines. \\
Unintuitive or non-descriptive names (i.e. the main concern of the project) can be the result of several actions either intentional (e.g. different names created by attackers to disguise the actual content of a file/document before exfiltration) or something that appears organically as part of the using system (e.g. results from running of scripts and experiments that use temporary file names which are not deleted; produced by the simulated developers while downloading documentation from the Internet).

In terms of privacy, although everything is made to look similar to real user behaviour, these traces are not genuine activities and no sensitive data is involved. The traces will be made public, hence there is no expectation of privacy. Similar techniques like those used in this project can perhaps be applied on real users and documents, but this has much tougher privacy/ethics implications. \\


\subsubsection*{Overview}

The first part of the project consists of classifying the files into appropriate categories using machine learning techniques. The machine learning model(s) used will have to take into account both the structure/content of the files and the structure of the graph(i.e. the context in which the file is used or was created). With regards to the context, it refers to a full provenance graph, rather then just to the program that created the file or the user that writes to the file. The second part of the project implies implementing the re-naming of files based on user-given policy and on-the-fly. Also, another target is to determine a more intuitive organization of the files in the file system, taking into account the obtained classification. Furthermore, the project will present the user a virtual file system in which the files appear with different names and possibly in a different hierarchy (rather than directly making changes to them). \\


\subsubsection*{Machine Learning Motivation}

The point of the project is to explore how/with what degree of success can machine learning be deployed to deal with the naming/structuring issues mentioned above. It is true that there is less variation in the data than a human running a system over a long period of time, and in that sense the problem is simpler. However, not applying machine learning means one has to know a lot more about what executes on the system and the types of files or the types of activity that generates them. The exercise here is to see how well a ML algorithm can start reducing the amount of knowledge needed from analysts when looking at an arbitrary system that could have been attacked.

In short, it's about a trade-off: one can apply non-ML approaches on this particular dataset if you start by knowing the ground truth or the types of activities that run on the system, but it's unclear whether a ML approach could do just as well while requiring less anterior knowledge. The goal is to design a ML pipeline to test this hypothesis out and give an answer to the question. \\

\section*{Starting point}

The prior knowledge and existing libraries that will be used for this project are listed below: \\

\begin{itemize}
  \item \textbf{Computer Science Tripos Courses}

        \begin{itemize}
          \item \textbf{Part IA: Machine learning and real world data} \\
                The Naive Bayes classifier built as practical exercise for this course is not actually relevant for this project. However, the basic concepts of training and test sets, as well as the evaluation methods (such as cross-validation) and metrics (such as precision, recall, accuracy) are.
          \item \textbf{Part IA: Databases} \\
                From this course I have obtained basic knowledge about graph databases which is relevant for this project.
          \item \textbf{Part IB: Software Engineering and Security}
                Good practices and approaches for building complex system were taught in this course. \\

        \end{itemize}

  \item \textbf{Open Source Libraries / APIs}

        \begin{itemize}
          \item \textbf{TensorFlow} \\
                Open source software library for high performance numerical computation.It comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains.

          \item \textbf{Keras} \\
                It is a high-level neural networks API, written in Python and capable of running on top of TensorFlow. It supports both convolutional networks and recurrent networks, as well as combinations of the two. \\
        \end{itemize}

  \item \textbf{Programming experience} \\
        I only have Java and C/C++ programming experience from previous years tripos courses. Hence, I will probably have to get used to Python for at least some parts of the project. \\

\end{itemize}

\section*{Resources required}

For this project I shall use my own laptop (i7-6700HQ CPU 2.60GHz, 16.0GB RAM, 500GB SSD, Windows 10 Home). \\
As backup, I will regularly push the code to a Github repository and upload any documents on Google Drive. I will also save all the files related to the project to an external hard disk. \\
Lastly, I will need a CL account in order to perform processing on a larger dataset. The required computing resources (servers) will be provided by my supervisor. \\


\section*{Work to be done}

The project plan will be split into the following main steps:\\

\begin{itemize}

  \item \textbf{Preparation} \\
        Books and academic papers reading will be required in the first place in order to be able to choose appropriate machine learning models that suit the needs of the project. I will also need time to getting used to Python and TensorFlow, perhaps by going through some online tutorials.\\

  \item \textbf{Get aquainted with the dataset provided} \\
        At the beginning of the project, I will have to understand exactly how the data is structured and how I can manipulate it. Afterwards, I need to process it in the sense that I need to extract the nodes of interest (i.e. the files) and to find a way to interpret the topology of the graph (i.e. the interactions between users/processes and files). \\

  \item \textbf{Establish a set of ground truths} \\
        In order to be able to apply appropriate supervised learning algorithms and  evaluate them, each file needs to have a category label attached. These labels will be determined by using a set of rules/ground truths. Knowing the data-generating process (which is the actual ground truth), one can generate a rule-based program that will automatically classify the files correctly. While there might be some thought given for naming the classes of files in the ground truth, subjective knowledge about files and their content is not required when assigning them to a class.
        Also, to avoid annotator bias, my supervisor offered to provide ground truth labels (using the approach described above) for commonly-agreed automated scenarios.\newpage

  \item \textbf{Feature engineering} \\
        The raw dataset needs to be transformed into flat features which can be used into a machine learning model.\\

  \item \textbf{Implement a ML system}\\
        I will be implementing a modified version of the Patchy-San algorithm (\url{https://arxiv.org/pdf/1605.05273.pdf}) that suits the requirements of my dataset. Patchy-San is a deep learning algorithm for recognizing patterns in directed or undirected graphs. Given an input graph, it performs several steps in order to obtain a feature vector which is then used as input for a convolutional neural network. However, my implementation will combine the graph-transformations with other information about the files (i.e. content) so that the features won't come only from the graph structure. \\
        It is worth noting that I will use off-the-shelf implementation only for some smaller components such as those required by various CNN layers, not for the entire system. \\

  \item \textbf{Model training} \\
        Train the implemented machine learning model with the labeled dataset previously created. Hence, obtain the intended classification. \\

  \item \textbf{Renaming and creating a virtual file space} \\
        Implement renaming based on user-given policy and on-the-fly. Create a virtual file space in which files appear with different names and possibly different hierarchy, which will be presented to the user. Since the file hierarchy might be different and there will also be new names, the matching between files in the new format and the old one should somehow be visible to the user. One way of doing this would be that the new file system will contain symbolic links to the original files. \\

  \item \textbf{Project evaluation} \\
        The evaluation will be done quantitatively, analyzing the performance of the built system in terms of metrics like accuracy, precision, recall, etc. Moreover, I shall analyze the influence of the provenance graph in the classification (with some metric I shall develop).\\

  \item \textbf{Dissertation} \\
        Write a dissertation which presents the work done and includes a discussion about the obtained results. Although the five chapters will be properly written as a final step of the project, notes of the main ideas and information about how the project is carried out will be written as the project is implemented.

\end{itemize}



\section*{Success criteria}

The project will be successful if the following objectives are achieved: \\

\begin{itemize}

  \item develop and optimize a machine learning pipeline that can use both provenance data and content data in order to do the file classification

  \item perform a quantitative evaluation of the implemented architecture(s) and investigate the best trade-off of precision and recall

  \item investigate the performance of the implemented pipeline in terms of time and space requirements \\


\end{itemize}


\section*{Possible extensions}

Provided I finish the core work on this project early, I will try to implement the following extensions: \\

\begin{itemize}
  \item \textbf{Further investigation} \\
        Do an investigation into what types of data/metadata are the most useful in classifying files. I.e. can the classification be made on provenance structure alone? Are node properties beside label and name of any importance? Are edge properties important? What if we only do classification based on metadata? (i.e. without looking at the file content). Comparisons between different classification approaches shall be done by using the evaluation metrics discussed above. \\

  \item \textbf{Build an UI} \\
        It would be possible and great to have an user interface since the renaming is done by considering an user-given policy. However, for the core part of the project I will use perhaps a configuration string passed as an option when mounting the virtual file system.
\end{itemize}


\section*{Timetable}

The project work will start on 20/10/2018, after the project proposal is submitted, and will be divided in 11 two/three-week slots and one final backup slot as follows: \\

\begin{itemize}
  \item \textbf{I. 20/10/2018 - 02/11/2018}

        \begin{itemize}

          \item do all the preliminary reading of books/papers/online materials about machine learning techniques that
                would suit the requirements of the project

        \end{itemize}

        \textbf{Milestones}: have a good understanding of the researched concepts and identify appropriate machine learning algorithms which can be used in the project\\


  \item \textbf{II. 03/11/2018 - 16/11/2018}

        \begin{itemize}

          \item decide which machine learning models to use
          \item familiarize with the given dataset
          \item familiarize with Python and its API for TensorFlow

        \end{itemize}

        \textbf{Milestones}: have a good understanding of: what I will implement in the project and how, how to use TensorFlow and how the given data can be processed\\




  \item \textbf{III. 17/11/2018 - 30/11/2018}

        \begin{itemize}

          \item decide on the features that should be extracted from the dataset in order to be used into the machine learning model(s)
          \item process the dataset and extract the chosen features

        \end{itemize}

        \textbf{Milestones}: have the feature engineering part of the project completed\\



  \item \textbf{IV. 08/12/2018 - 28/12/2018}

        \begin{itemize}
          \item implement the actual machine learning model(s)
          \item perform unit testing on the model(s)
        \end{itemize}

        \textbf{Milestones}: have a fully operational machine learning model implemented\\



  \item \textbf{V. 05/01/2019 - 25/01/2019}

        \begin{itemize}
          \item train the machine learning model
          \item test and debug the implementation up to the given stage of the project
          \item write progress report
          \item prepare slides for presentation
        \end{itemize}

        \textbf{Milestones}: have the progress report and presentation completed and have a working implementation up until this point\\


  \item \textbf{VI. 26/01/2019 - 08/02/2019}

        \begin{itemize}
          \item build a virtual file system
          \item integrate the file system into the project
        \end{itemize}

        \textbf{Milestones}: have a fully functional virtual file system which gives the user a preview of the new files' naming and possible different hierarchy\\



  \item \textbf{VII. 09/02/2019 - 22/02/2019}

        \begin{itemize}
          \item test and debug the code
        \end{itemize}

        \textbf{Milestones}: have the implementation phase completed \\



  \item \textbf{VIII. 23/02/2019 - 15/03/2019}

        \begin{itemize}
          \item identify appropriate metrics on which to evaluate the project (e.g. accuracy, precision, recall)
          \item also evaluate the project quantitatively in terms of time and space requirements
          \item tune the machine learning pipeline to improve evaluated metrics

        \end{itemize}

        \textbf{Milestones}: successfully evaluate the project as described above, also investigating the best trade-off of recall and precision \\



  \item \textbf{IX. 16/03/2019 - 29/03/2019}

        \begin{itemize}
          \item start writing the dissertation
        \end{itemize}

        \textbf{Milestones}: have the Introduction, Preparation and Conclusion Chapters completed and sent for feedback\\



  \item \textbf{X. 30/03/2019 - 12/04/2019}

        \begin{itemize}
          \item continue working on dissertation
        \end{itemize}

        \textbf{Milestones}: have the Implementation and Evaluation chapters completed and sent for feedback\\




  \item \textbf{XI. 13/04/2019 - 26/04/2019}

        \begin{itemize}
          \item Based on the feedback from my supervisor, edit the dissertation by making all the suggested changes
        \end{itemize}

        \textbf{Milestones}: have the dissertation completed for submission\\


  \item \textbf{XII. 27/04/2019 - 17/05/2019}

        \begin{itemize}
          \item this period will be dedicated to the situation in which something unexpected occurs and the work on the project is not completed by the scheduled time
        \end{itemize}

        \textbf{Milestones}: send the final version of the dissertation before the deadline

\end{itemize}



\end{document}